---
title: "Why Don't They Show Up?"
output: 
  pdf_document:
    number_sections: false
    toc: false
    keep_tex: true
fontfamily: times
fontsize: 12pt
bibliography: mooc-refs.bib
csl: international-journal-of-research-in-marketing.csl

header-includes:
  - \usepackage[nofiglist, nofighead]{endfloat}  
  - \usepackage[singlespacing]{setspace}
  - \usepackage{sectsty} \sectionfont{\centering}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'p') # Places figures on their own pages
knitr::opts_chunk$set(out.width = '100%', dpi=300)
knitr::opts_chunk$set(echo = FALSE)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(lubridate)
library(janitor)
library(here)
library(feather)

library(viridis)
library(ggthemes)
library(gridExtra)
library(scales)
library(knitr)

theme_set(theme_minimal())

DATA_LOC <- here("paper", "data", "full-mooc-data.csv")
```

# ABSTRACT

Massive Open Online Courses (MOOCs) have gained prominence and exponential growth over the last five years. Despite the high number of registered participants, MOOCs are plagued by a high rate of non-completion. In this study, we focus on crytallizing features that are predictive of initial dropout, i.e., the situation where participants register but don’t commence the course. Using a Gradient Boosting Model we show that while early registration is a key predictor of initial dropout, participant demographics and course content are relatively unimportant. Our results indicate that early registrants need close monitoring and resources should be allocated to monitor and nudge participants who are at risk of dropout to begin the MOOC. Beyond the practical significance of our findings in marketing MOOCs, we present a novel amalgamation of feature engineering based on consumer theory and powerful machine learning methods.

*Keywords*: MOOCs, Initial dropout, Customer impatience, Gradient boosted trees

\newpage

# INTRODUCTION

Massive Open Online Courses (MOOCs) have received widespread attention since their launch in 2012. Since then, MOOCs evolved from being largely free-to-access to a pay-for-certification model. For e.g., 78 million learners took part in MOOCs in 2017, with the proportion of participants paying for courses increasing over previous years [@peters2018moocsevolved]. Several local universities and governments have also stepped in to the fray by offering several of their courses online, where participants might get a certificate based on their performance in the course for a fee. Over the past couple of years MOOCs have even evolved into public policy initiatives with the aim of upskilling the labor force. A good example of such a effort is Swayam - a collaborative effort between the Government of India and several top universities in India [@bast2018swayam]. 

In this paper, we focus on measuring and predicting the *dropout rate*, i.e., the fraction of participants who enroll for a MOOC but do not finish with a certification. We note that dropout rates are deal-breakers for government efforts like Swayam which might get derailed by high dropout rates. Similar is the case of small private online courses offered to corporations by universities, where high initial dropout rate can be devastating. Following this intuition, a central theme of our proposed research is the *initial dropout rate*, measured as the number of participants who register for a course but don’t watch a single video.

The rest of the paper is organized as follows. Section 2 presents a review of existing literature, and our central hypotheses on predictors of initial drop-out. Section 3 presents the details of our predictive models and results. The paper concludes with a discussion on the implications of this study and a proposal for further research.  

# ANALYZING INITIAL DROPOUT

Dropout rates in MOOCs are a widely researched area within the machine learning community. An overview of key themes that emerge from prior research is presented in the next sub section.

## Related Research

As the initial euphoria on MOOCs settles down, significant research interest is being focused on peculiar aspects of participant behavior in these courses [@kross2018students]. A defining feature of MOOCs is the noticeably low certification rate (usually less than 4%) across courses and providers [@onah2014dropout]. There are two arguments proposed by scholars to address this concern. First, there is a wide gamut of participants who enroll in MOOCs and looking only at certification rate of a course would not do justice to the utility gained by a participant from a MOOC. For e.g., @belanger2013bioelectricity show that the utility of participating in a MOOC goes beyond the attainment of certification and encompasses a quest to understand a subject, fun, convenience or even exploration of a new learning medium. Second, even though the number of certifications is less in terms of percentages, absolute numbers are still many multiples of the number of students who complete a typical university course [@kizilcec2015attrition]. This observation is often cited as justification for the investments made into creating and promoting MOOCs.  

Another area of active research is on the non-completion of courses, where some scholars identify the unique aspects of MOOCs as prime drivers for non-completion. For e.g., in an analysis of factors that predict completion of a course, @yang2013turn argue that MOOCs have a unique development history. Starting from a small participant base upon announcement, new cohorts join in week after week. Consequently, the authors argue that if supportive communities do not evolve as the course progresses, participants might feel overwhelmed and drop-out. 

A typical method used by scholars in predictive models of dropout is to utilize observable aspects of a participants behavior drawn from click stream data as features [@whitehill2017delving]. However, most research attention has been focused on predicting the grades earned by participants who earn certificates and there are no studies that build predictive models of initial dropout.  

## Data

Our research is based on the analysis of a large data set of $476,532$ participants who enrolled for 16 Harvard and MIT MOOCs for the period 2012-2013. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
mooc_df <- read_csv(DATA_LOC)
```

```{r}
mooc_df %>% 
  
  select(gender) %>% 
  
  drop_na() %>% 
  
  filter(gender %in% c('m', 'f')) %>% 
  
  count(gender) ->
  
  gender_distribution_df

ggplot(gender_distribution_df) +
  geom_bar(aes(x = reorder(gender, n), y = n), 
           stat = 'identity',
           color = 'white',
           fill = 'black') +
  labs(x = 'Gender',
       y = 'Count',
       title = 'Participant distribution by gender') +
  scale_x_discrete(labels = c('Female', 'Male')) +
  scale_y_continuous(labels = comma) +
  coord_flip() ->
  gender_plot
```

```{r}
mooc_df %>% 
  
  mutate(country = case_when(final_cc_cname_DI == "United States" ~ "USA",
                             final_cc_cname_DI %in% c("India", "Pakistan", 
                                                      "Bangladesh", "China",
                                                      "Indonesia", "Japan", 
                                                      "Other East Asia", "Other Middle East/Central Asia",
                                                      "Other South Asia", "Philippines",
                                                      "Egypt") ~ "Asia",
                             final_cc_cname_DI %in% c("France", "Germany", 
                                                      "Greece", "Other Europe",
                                                      "Poland", "Portugal", 
                                                      "Russian Federation", "Spain",
                                                      "Ukraine", "United Kingdom") ~ "Europe",
                             final_cc_cname_DI %in% c("Morocco", "Nigeria", 
                                                      "Other Africa") ~ "Africa",
                             TRUE ~ "Other")) %>% 
  
  count(country) %>% 
  
  select(country, n) ->
  
  location_distribution_df

ggplot(location_distribution_df) +
  geom_bar(aes(x = reorder(country, n), y = n), 
           stat = 'identity',
           color = 'white',
           fill = 'black') +
  labs(x = 'Country',
     y = 'Count',
     title = 'Participant distribution by location') +
  scale_y_continuous(labels = comma) +
  coord_flip() ->
  location_plot

```

```{r fig.cap="Descriptive statistics \\label{descriptive_plots}", fig.align='center'}

grid.arrange(gender_plot, location_plot, nrow = 2)

```


As the descriptive statistics in Figure \ref{descriptive_plots} indicate, majority of the participants are male. Also, USA and Asia contribute most to the number of participants (note that we group participants in the data set by continent).

To characterize initial dropout on MOOCs, we define a participant as ‘engaged’ if they register and view at least one video from the course (it follows that those who browsed more than one video and those who earned certificates are also classified as ‘engaged’). Similarly, a participant is classified as ‘not engaged’ if they register but never turn up, i.e., they dropout even before starting the course. Figure \ref{activity_status} summarizes the distribution of participants into these two categories across the 16 courses. 

```{r fig.cap="Activity Status \\label{activity_status}", fig.align='center', out.height='100%'}
mooc_df %>% 
    
  mutate(engaged = ifelse(viewed == 1 | explored == 1 | certified == 1, 1, 0),
         status = ifelse(engaged == 1, "Engaged", "Not Engaged")) %>% 
  
  group_by(course_id, status) %>% 
  
  summarize(count = n()) ->
  
  initial_dropout_df

initial_dropout_df %>% 
    
    spread(key = status, value = count) %>% 
    
    mutate(perc_engaged = Engaged * 100/(Engaged + `Not Engaged`)) ->
    
    perc_initial_dropout_df


ggplot(initial_dropout_df) +
  geom_bar(aes(x = course_id, y = count, fill = status), 
           stat = "identity", 
           position = position_dodge()) +
  labs(x = "Course",
       y = "Number of participants",
       title = "Activity status across all courses") +
  scale_fill_grey("Course Status") +
  scale_y_continuous(labels = comma) +
  theme(legend.position="bottom") +
  coord_flip() 
```

Since our objective is to predict initial dropout, our data is limited to the behavior of participants before the course begins. We note that this is in contrast to predictive models of grades or non-completion in prior research, where significantly more data is available (e.g., number of chapters accessed, number of videos, discussion forum activity). While the lack of rich data poses challenges in model building, our endeavor is to select a model that is most accurate in predicting dropout given the data.

## Predicting Initial Dropout

We note from Figure  \ref{activity_status} that for every course, the ‘not engaged’ category presents a significant challenge. On an average, the initial drop-out rate on the Harvard and MIT MOOCs is ```r paste0(100 - round(mean(perc_initial_dropout_df$perc_engaged), 1), '%') ```, which is disconcerting in the context of our earlier discussion on public policy initiatives.  

In this study, we conceptualize the MOOC participant as a consumer of course information. The purchase (i.e., course registration) is done in advance but the product (i.e., the course) is delivered at a later date. In the context of initial dropout, we argue that there are three factors that might lead early registrants to dropout before the course begins. First, at the point of registration, participants are excited by the content of the course and commit to begin the course. However, since the course is delivered at a later stage, the tangible benefits from course completion accrue only in the future. This is a key facet of the current design of MOOCs and is closely related to theories of customer impatience in the digital world. In this body of research the central argument is that one a purchase is made, informed, fully visible and on-time delivery process is the new norm [@daugherty2018impatience]. Customers are willing to accept delayed outcomes only if a longer waiting time results in a higher future value [@marino2018consumer]. Further, for lower-valued outcomes, time sensitivity (i.e., the value placed on time) is higher [@thaler1981some]. Translating these findings to the MOOC environment, we argue that while participants are aware of the timelines of course delivery at the point of registration, activity commences only close to the launch date of the course. This lack of visible activity leads to an unresolved impatience. The issue is exacerbated by the fact that growth spurts and attrition that characterize the run-up to start of a course make community formation difficult [@yang2013turn]. The situation is especially dire for participants whose motivation to join the MOOC is not certification but fun or convenience [@belanger2013bioelectricity]. For these participants, the innate desire that drives them to register might already be quenched by the act of registration. When the course starts at a later stage, this event has much lesser utility. Following these arguments, we operationalize customer impatience as the waiting time of participants till the commencement of the course and compute this as the lead time between the date of registration of the participant and the start date of the course. We expect that the date of registration, i.e., whether the participant registered early or late relative to the course start date might be strongly predictive of initial dropout. 

Second, a peculiar aspect of the data set is that the content of courses within the data set is varied (for e.g., the courses include Ancient Greek Hero and Introduction to Solid State Chemistry) and draws a wide range of participants[@ho2014harvardx]. Prior research indicates that the completion rates on MOOCs are low across different course types [@onah2014dropout]. In this research, we propose to validate this finding in the case of initial drop-out as well. Consequently, we expect the course content to have no effect on the willingness of the participants to attend the course.

Third, we expect participant demographics to be reflective of the infrastructure available in order to successfully engage with a MOOC. For e.g., a participant from United States is expected to have access to better internet facilities compared to a participant from Rwanda.

Following the descriptive measures and the arguments presented in this section, we summarize our exploration of initial drop-out into the following research questions:

*RQ1: Early registration is strongly predictive of initial drop-out* 

*RQ2: The subject of the course is not an important predictor of initial drop-out* 

*RQ3: Demographics of a participant are strongly predictive of initial drop-out* 

# MODELING INITIAL DROPOUT

In this section, we present predictive models for initial drop-out using 3 methods - Logistic Regression, Gradient Boosting and Neural Networks. 

## Preprocessing

Engagement status was used as the label (coded 1 for ‘engaged’ and 0 for ‘not engaged) to be predicted in all our models. The features used to predict the engagement status were extracted from age, gender, country and date of registration. Following the discussion in section 2.3, we added a new variable `joined_early_or_late` by subtracting the start date of a course from the date of registration by the participant. We expect this variable to be the key predictor in our model (RQ1). The other key variables included were age, gender and country (one-hot encoded). Finally, we excluded the participants who registered for more than one course (this allows us to justify the independent samples assumption underlying the models). The final sample size on which the models were evaluated is $n = 316,917$.

## Model Execution

Since we want our models to be strongly predictive of drop-out, we attach prime importance to misclassified participants. In particular, we want the number of participants who were misclassified as non drop-outs to be minimum. To enforce this we scored our models using the AUC of the ROC curve as the metric and selected the final model hyper parameters based on 10-fold cross validation with 3 repeats. To account for the class imbalance we observe in Figure \ref{descriptive_plots}, we follow prior literature and employ the Synthetic Minority Over-sampling Technique (SMOTE) to form the training set (chosen as 80% of the entire data set).

Logistic regression was executed using L2-regularization with the regularization strength as the hyper parameter. For Gradient Boosting, we use the `xgBoost` algorithm [@chen2016xgboost], with depth of the trees and the number of estimators tuned as hyper parameters during training (we performed a random grid search over a larger parameter space to arrive at a shortlist). The neural networks were composed of a sequence of fully connected layers with the number of units per layer and the number of layers tuned during training. The final model comprised 9 fully connected layers, with 32 units in each layer (total number of trainable parameters $=4,417$).   

## Results

```{r fig.cap="ROC curves for the 3 model fits \\label{roc_curves}", fig.align="center", out.width="100%"}

logit_roc_df <- read_feather("../data/logit-roc.feather")
nnet_roc_df <- read_feather("../data/nnet-roc.feather")
xgb_roc_df <- read_feather("../data/xgb-roc.feather")

logit_roc_df %>% mutate(model = "Logistic Regression") -> logit_roc_df
nnet_roc_df %>% mutate(model = "Neural Network") -> nnet_roc_df
xgb_roc_df %>% mutate(model = "Gradient Boosting") -> xgb_roc_df

all_roc_df <- bind_rows(logit_roc_df, nnet_roc_df, xgb_roc_df)

ggplot(all_roc_df, aes(x = false_positive_rate, y = true_positive_rate)) +
    geom_step(aes(color = model)) +
    geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +
    scale_colour_viridis_d() +
    labs(x = "False Positive Rate",
         y = "True Positive Rate",
         color = "Model",
         title = "ROC curves for the three models")
```

As can be inferred from \ref{roc_curves}, Gradient Boosting and Neural Networks perform much better than Logistic Regression. However, since the Gradient Boosting model had a higher accuracy on the test set (76.1%) we choose this as our final model. 

# Discussion

The results presented in Section 3 indicate that the Gradient Boosted Model is a good model for the data set. In order to address the research questions posed in Section 2, we now move to probe the relative importance of the factors in the predictive ability of the model on the data set. Relative importance of a feature is computed by averaging (across all trees) the number of times this feature is selected for the split weighted by the improvement to the resulting model [@elith2008working]. Figure \ref{feature_importance} shows the relative importance of the top 5 features that contribute most to the predictive ability of the final Gradient Boosting Model on the test set. 

```{r fig.cap="Feature importance plot of the Gradient Boosted Model \\label{feature_importance}", fig.align="center", out.width="100%"}

xgb_feature_imp_df <- read_feather('../data/xgb-feature-importance.feather')

xgb_feature_imp_df %>% 
    
    top_n(5, feature_importance) %>% 
    
    arrange(desc(feature_importance)) ->
    
    xgb_top_five_features_df

ggplot(xgb_top_five_features_df) +
    geom_bar(aes(x = reorder(feature, feature_importance), y = feature_importance), 
             stat = 'identity',
             width = 0.5,
             color = 'white',
             fill = 'black') +
    coord_flip() +
    labs(x = 'Feature',
         y = 'Feature Importance',
         title = 'Top 5 features for the Gradient Boosted Model')
```

Figure \ref{feature_importance} indicates that the most important feature to the prediction of initial drop-out is whether the participant joined early (validating RQ1). Also, the course of study is not an important predictor for initial drop-out validating RQ2. Further, contrary to our expectation summarized in RQ3, our results indicate that education and country are not on the same scale of importance as early registration. This is surprising since we expect participants with higher education levels to be more disciplined in attending and completing MOOCs (given that they have attended and completed courses within their formal education). The emergence of age as a stronger predictor is a surprising finding we wish to explore in further research. 

In sum, we conclude that early registration is a key predictor of initial dropout. This has important implications for the marketing and execution of MOOCs. In practice, we observe that little attention is paid to monitoring of participants before the course begins and our results indicate that neglecting this phase has a devastating effect on the dropout rate. Our results indicate that early registrants need to be watched carefully and nudged to begin the course. We submit that this is paramount for government initiatives, and we strongly advocate the allocation of resources and attention to participants who have registered early. Our results also indicate that the course of study is not predictive of initial drop-out, and we propose that decision makers do not make any distinction among courses while allocating resources. It is important to note that while the precise initial dropout rates might differ across courses, the subject of the course itself is not predictive of initial dropout. 

This study makes several important contributions to existing literature on MOOCs. First, our conceptualization of a MOOC participant as a consumer allows us to draw upon prior research on consumer behavior to choose predictive features for our machine learning algorithms. We submit that our approach to predictive modeling is novel in this aspect - while our methods are rooted in data mining, the choice of predictive features in these models is guided by consumer research. Second, our results crystallize key factors that are predictive of initial drop-out and can aid decision makers to market MOOCs effectively. Third, our final model can be used to flag participants who are most likely to drop out before the course begins. Our proposal is to utilize the predictions from our model to fine tune marketing efforts directed at reduction in the initial drop-out rate. All the code used for the modeling and data processing is available in a `git` repository and can be replicated by other scholar/practitioners to design MOOCs.    

Since this study is one of the first to probe initial drop-out, it is limited by the analysis of the secondary data. However, by using powerful machine learning methods, we are able to crystallize key features decision makers should be aware of while launching MOOCs. In the next phase of research, we wish to undertake primary research to understand the decision-making process of MOOC drop-outs and incorporate the findings from this research into better predictive models of initial dropout. 

\newpage 

# References
