---
title: "Why Don't They Show Up?"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document:
    number_sections: true
    toc: false
fontfamily: times
fontsize: 12pt
bibliography: mooc-refs.bib
csl: international-journal-of-research-in-marketing.csl

header-includes:
  - \usepackage[nofiglist, nofighead]{endfloat}  
  - \usepackage{setspace}\singlespacing
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'p') # Places figures on their own pages
knitr::opts_chunk$set(out.width = '100%', dpi=300)
knitr::opts_chunk$set(echo = FALSE)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(lubridate)
library(janitor)
library(here)
library(feather)

library(viridis)
library(ggthemes)
library(gridExtra)
library(scales)
library(knitr)
library(kableExtra)

theme_set(theme_minimal())

DATA_LOC <- here("paper", "data", "full-mooc-data.csv")
```

# Introduction

Massive Open Online Courses (MOOCs) have received widespread attention since their launch in 2012. Since then, MOOCs evolved from being largely free to access to a pay-for-certification model. For e.g., 78 million learners took part in MOOCs in 2017, with the proportion of participants paying for courses increasing over previous years [@peters2018moocsevolved]. Several local universities and governments have also stepped in to the fray by offering several of their courses online, where participants might get a certificate based on their performance in the course for a fee. Over the past couple of years MOOCs have even evolved into public policy initiatives with the aim of upskilling the labor force. A good example of such a effort is Swayam - a collaborative effort between the Government of India and several top universities in India [@bast2018swayam]. 

In this paper, we focus on measuring and predicting the *drop-out rate*, i.e., the fraction of participants who enroll for a MOOC but do not finish with a certification. We note that drop-out rates are deal-breakers for government efforts like Swayam which might get derailed by high drop-out rates. Similar is the case of small private online courses offered to corporates by universities, where high initial drop-out rate can be devastating. Following this intuition, a central theme of our proposed research is the *initial drop-out rate*, measured as the number of participants who register for a course but don’t even watch a single video.

The rest of the paper is organized as follows. Section 2 presents a review of existing literature, and our central hypotheses on predictors of initial drop-out. Section 3 presents the details of our predictive models and results. The paper concludes with a discussion on the implications of this study and a proposal for further research.  

# Analyzing initial dropout

Dropout rates in MOOCs are a widely researched area within the machine learning community. Much of this research relies on access to fine-grained data on participant behavior captured via clickstreams. We present an overview of key themes that emerge from prior research in the next sub section.

## Related research

As the initial euphoria on MOOCs settles down, significant research interest is being focused on peculiar aspects of participant behavior in these courses [@kross2018students]. A defining feature of MOOCs is the noticeably low certification rate (usually less than 4%) across courses and providers [@onah2014dropout]. There are two arguments proposed by scholars to explain this observation. First, there is a wide gamut of participants who enroll in MOOCs and looking only at certification rate of a course would not do justice to the utility gained by a participant from a MOOC. For e.g., @belanger2013bioelectricity show that the utility of participating in a MOOC goes beyond the attainment of certification and encompasses a quest to understand a subject, fun, convenience or even exploration of a new learning medium. Second, even though the number of certifications is less in terms of percentages, absolute numbers are still many multiples of the number of students who complete a typical university course [@kizilcec2015attrition]. This observation is often cited as justification for the investments made into creating and promoting MOOCs. We submit that since MOOCs are designed to have low barriers to both entry and exit, a wide range of participant behavior can be observed. 

Researchers have tackled dropout by modeling it as the dependent variable predicted by observable aspects of a participants behavior relying on access to clickstream data [@whitehill2017delving]. However, most research attention has been focused on predicting the grades earned by participants who earn certificates. This focus on the paying segment is understandable since for-profit MOOC platforms (e.g., Coursera, edX) have limited resources that are exhausted once they cater to this segment. 

## Data

Our research is based on the analysis of a large data set of $476,532$ participants who enrolled for 16 Harvard and MIT MOOCs for the period 2012-2013. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
mooc_df <- read_csv(DATA_LOC)
```

```{r}
mooc_df %>% 
  
  select(gender) %>% 
  
  drop_na() %>% 
  
  filter(gender %in% c('m', 'f')) %>% 
  
  count(gender) ->
  
  gender_distribution_df

ggplot(gender_distribution_df) +
  geom_bar(aes(x = reorder(gender, n), y = n), 
           stat = 'identity',
           color = 'white',
           fill = 'black') +
  labs(x = 'Gender',
       y = 'Count',
       title = 'Participant distribution by gender') +
  scale_x_discrete(labels = c('Female', 'Male')) +
  scale_y_continuous(labels = comma) +
  coord_flip() ->
  gender_plot
```

```{r}
mooc_df %>% 
  
  mutate(country = case_when(final_cc_cname_DI == "United States" ~ "USA",
                             final_cc_cname_DI %in% c("India", "Pakistan", 
                                                      "Bangladesh", "China",
                                                      "Indonesia", "Japan", 
                                                      "Other East Asia", "Other Middle East/Central Asia",
                                                      "Other South Asia", "Philippines",
                                                      "Egypt") ~ "Asia",
                             final_cc_cname_DI %in% c("France", "Germany", 
                                                      "Greece", "Other Europe",
                                                      "Poland", "Portugal", 
                                                      "Russian Federation", "Spain",
                                                      "Ukraine", "United Kingdom") ~ "Europe",
                             final_cc_cname_DI %in% c("Morocco", "Nigeria", 
                                                      "Other Africa") ~ "Africa",
                             TRUE ~ "Other")) %>% 
  
  count(country) %>% 
  
  select(country, n) ->
  
  location_distribution_df

ggplot(location_distribution_df) +
  geom_bar(aes(x = reorder(country, n), y = n), 
           stat = 'identity',
           color = 'white',
           fill = 'black') +
  labs(x = 'Country',
     y = 'Count',
     title = 'Participant distribution by location') +
  scale_y_continuous(labels = comma) +
  coord_flip() ->
  location_plot

```

```{r fig.cap="Descriptive statistics \\label{descriptive_plots}", fig.align='center'}

grid.arrange(gender_plot, location_plot, nrow = 2)

```


As the descriptive statistics in Figure \ref{descriptive_plots} indicate, most of the participants are male. Also, USA and Asia contribute most to the number of participants.

To characterize initial drop-out on MOOCs, we define a participant as ‘engaged’ if they register and view at least one video from the course (it follows that those who browsed more than one video and those who earned certificates are also classified as ‘engaged’). Similarly, a participant is classified as ‘not engaged’ if they register but never turn up, i.e., they drop-out even before starting the course. Figure \ref{activity_status} summarizes the distribution of participants into these two categories across the 16 courses. 

```{r fig.cap="Activity Status \\label{activity_status}", fig.align='center', out.height='100%'}
mooc_df %>% 
  mutate(engaged = ifelse(viewed == 1 | explored == 1 | certified == 1, 1, 0),
         status = ifelse(engaged == 1, "Engaged", "Not Engaged")) %>% 
  group_by(course_id, status) %>% 
  summarize(count = n()) ->
  initial_dropout_df

ggplot(initial_dropout_df) +
  geom_bar(aes(x = course_id, y = count, fill = status), 
           stat = "identity", 
           position = position_dodge()) +
  labs(x = "Course",
       y = "Number of participants",
       title = "Activity status across all courses") +
  scale_fill_grey("Course Status") +
  scale_y_continuous(labels = comma) +
  theme(legend.position="bottom") +
  coord_flip() 
```

## Predicting initial dropout

We note from Figure  \ref{activity_status} that for every course, the ‘not engaged’ category presents a significant challenge. On an average, the initial drop-out rate on the Harvard and MIT MOOCs is about 50%, which is disconcerting in the context of our earlier discussion on public policy initiatives. While there is little research on the drivers of initial dropout, some scholars have pointed out the unique aspects of MOOCs as one of the prime reasons for dropout. For e.g., in an analysis of factors that predict completion of a course, @yang2013turn argue that MOOCs have a unique development history. Starting from a small participant base upon announcement, new cohorts join in week after week. Consequently, the authors argue that if supportive communities do not evolve as the course progresses, participants might feel overwhelmed and drop-out. 

In the context of initial dropout, we argue that there are two factors that might lead early registrants to drop out. First, the growth spurts and attrition that characterize the run-up to start of a course make comunity formation difficult [@yang2013turn]. Second, participants who register early might have done so in a moment of enthusiasm that dissipates by the time the course eventually starts. Following these arguments, we expect that date of registration, i.e., whether the participant registered early or late (relative to the course start date) might be strongly predictive of initial dropout. Similarly, we expect participant demographics to be reflective of the infrastructure available in order to successfully engage with a MOOC. For e.g., a participant from United States is expected to have access to better internet facilities compared to a participant from Rwanda.

Following the descriptive measures and the arguments presented in this section, the research questions we explore are:

*RQ1: Early registration is strongly predictive of initial drop-out*

*RQ2: The country of residence of a participant is strongly predictive of initial drop-out*

# Modeling initial dropout

In this section, we present predictive models for initial drop-out using 3 methods - Logistic Regression, Gradient Boosting and Neural Networks. 

## Preprocessing

Engagement status was used as the label (coded 1 for ‘engaged’ and 0 for ‘not engaged) to be predicted in all our models. The features used to predict the engagement status were extracted from age, gender, country and date of registration. Following the discussion in section 2.3, we added a new variable `joined_early_or_late` by subtracting the start date of a course from the date of registration by the participant. We expect this variable to be the key predictor in our model. The other key variables included were age, gender and country (one-hot encoded). Finally, we excluded the participants who registered for more than one course (this allows us to justify the independent samples assumption underlying the models). The final sample size on which the models were evaluated is $n = 316,917$.

## Model execution

Since we want our models to be strongly predictive of drop-out, we attach prime importance to misclassified participants. In particular, we want the number of participants who were misclassified as non drop-outs to be minimum. To enforce this we scored our models using the AUC of the ROC curve as the metric and selected the final model hyperparameters based on 10-fold cross validation with 3 repeats. To account for the class imbalance we observe in Figure \ref{descriptive_plots}, we follow prior literature and employ the Synthetic Minority Over-sampling Technique (SMOTE) to form the training set (chosen as 80% of the entire data set).

Logistic regression was executed using L2-regularization with the regularization strength as the hyperparameter. For Gradient Boosting, we use the `xgBoost` algorithm [@chen2016xgboost], with depth of the trees and the number of estimators tuned as hyperparameters during training (we performed a random grid search over a larger parameter space to arrive at a shortlist). The neural networks were composed of a sequence of fully connected layers with the number of units per layer and the number of layers tuned during training. The final model comprised 9 fully connected layers, with 32 units in each layer (total number of trainable parameters $=4,417$).   

## Results

```{r fig.cap="ROC curves for the 3 model fits \\label{roc_curves}", fig.align="center", out.width="100%"}

logit_roc_df <- read_feather("../data/logit-roc.feather")
nnet_roc_df <- read_feather("../data/nnet-roc.feather")
xgb_roc_df <- read_feather("../data/xgb-roc.feather")

logit_roc_df %>% mutate(model = "Logistic Regression") -> logit_roc_df
nnet_roc_df %>% mutate(model = "Neural Network") -> nnet_roc_df
xgb_roc_df %>% mutate(model = "Gradient Boosting") -> xgb_roc_df

all_roc_df <- bind_rows(logit_roc_df, nnet_roc_df, xgb_roc_df)

ggplot(all_roc_df, aes(x = false_positive_rate, y = true_positive_rate)) +
    geom_step(aes(color = model)) +
    geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +
    labs(x = "False Positive Rate",
         y = "True Positive Rate",
         color = "Model",
         title = "ROC curves for the three models")
```

As can be inferred from \ref{roc_curves}, Gradient Boosting and Neural Networks perform much better than Logistic Regression. However, since the Gradient Boosting model had a higher accuracy on the test set (76.1%) we choose this as our final model. 

# Discussion

The results presented in Section 3 indicate that the Gradient Boosted Model is a good model for the data set. In order to address the two research questions posed in Section 2, we now move to probe the relative importance of the factors in the predictive ability of the model on the data set. Relative importance of a feature is computed by averaging (across all trees) the number of times this feature is selected for the split weighted by the improvement to the resulting model [@elith2008working]. Figure \ref{feature_importance} shows the relative importance of the top 5 features that contribute most to the predictive ability of the final Gradient Boosting Model on the test set. 

```{r fig.cap="Feature importance plot of the Gradient Boosted Model \\label{feature_importance}", fig.align="center", out.width="100%"}

xgb_feature_imp_df <- read_feather('../data/xgb-feature-importance.feather')

xgb_feature_imp_df %>% 
    
    top_n(5, feature_importance) %>% 
    
    arrange(desc(feature_importance)) ->
    
    xgb_top_five_features_df

ggplot(xgb_top_five_features_df) +
    geom_bar(aes(x = reorder(feature, feature_importance), y = feature_importance), 
             stat = 'identity',
             width = 0.5,
             color = 'white',
             fill = 'black') +
    coord_flip() +
    labs(x = 'Feature',
         y = 'Feature Importance',
         title = 'Top 5 features for the Gradient Boosted Model')
```

Figure \ref{feature_importance} indicates that the most important feature to the prediction of initial drop-out is whether the participant joined early (validating RQ1). Further, contrary to our expectation summarized in RQ2, our results indicate that education and country are not on the same scale of importance as early registration. This is suprising since we expect participants with higher education levels to be more disciplined in terms of attending and completing MOOCs (given that they have attended and completed courses within their formal education). The emergence of age as a stronger predictor is a surprising finding we wish to explore in further research. 

In sum, we conclude that early registration is a key predictor of initial drop-out. This has important implications for the marketing and execution of MOOCs. In practise, we observe that little attention is paid to monitoring of participants before the course begins and our results indicate that neglecting this phase has a devastating effect on the drop-out rate. Our results indicate that early registrants need to be watched carefully and nudged to begin the course. We submit that this is paramount for government initiatives, and we strongly advocate the allocation of resources and attention to participants who have registered early.

Since this study is one of the first to probe initial drop-out, it is limited by the analysis of the secondary data. However, by using powerful machine learning methods, we are able to crystallize key features decicion makers should be aware of while launching MOOCs. In the next phase of research, we wish to undertake primary research to understand the decision-making process of MOOC drop-outs and incorporate the findings from this research into better predictive models of initial dropout. 

\newpage 

# References
